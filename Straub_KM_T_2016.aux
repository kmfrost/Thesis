\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Problem Overview}{2}{section.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Approach}{2}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Contributions}{3}{section.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Outline}{3}{section.1.5}}
\citation{radicati_emails_2015}
\citation{fisher_revisiting_2006}
\citation{klimt_introducing_2004}
\citation{gaber_e-mail_2016}
\citation{shams_classifying_2013}
\citation{he_novel_2014}
\citation{keila_structure_2005}
\citation{sofershtein_predicting_2015}
\citation{hu_towards_2012}
\citation{nordbo_data_2014}
\citation{waterman_big_2014}
\citation{namata_inferring_2006}
\citation{shetty_enron_2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Prior Work}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{PriorWork}{{2}{4}{Prior Work}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Background}{4}{section.2.1}}
\citation{tang_email_2013}
\citation{yelupula_social_2008}
\citation{martin_analyzing_2005}
\citation{wasserman_social_1994}
\citation{freeman_set_1977}
\citation{tyler_email_2003}
\citation{wilson_discovery_2009}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Commonly Used Datasets}{5}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Prior Analysis}{5}{section.2.3}}
\citation{rowe_automated_2007}
\citation{shetty_status_2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Email Dataset Feature Extraction}{7}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Data}{{3}{7}{Email Dataset Feature Extraction}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data Collection}{7}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces A comparison between the internal dataset and the Enron email corpus.\relax }}{8}{table.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:db_stats}{{3.1}{8}{A comparison between the internal dataset and the Enron email corpus.\relax }{table.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Pie chart showing the distribution of classes in the dataset.\relax }}{9}{figure.caption.7}}
\newlabel{fig:class_breakdown}{{3.1}{9}{Pie chart showing the distribution of classes in the dataset.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Dataset Description and Statistics}{9}{section.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Features}{10}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Traffic-Based Features}{10}{subsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Histogram of unique subjects received by job title. The feature value for this plot has been normalized so that all values fall between 0 and 1. Note that by using different thresholds, meaningful splits in the data can be made. For example, all but two graduate students have a value less than $0.1$.\relax }}{12}{figure.caption.8}}
\newlabel{fig:traffic_ex_hist}{{3.2}{12}{Histogram of unique subjects received by job title. The feature value for this plot has been normalized so that all values fall between 0 and 1. Note that by using different thresholds, meaningful splits in the data can be made. For example, all but two graduate students have a value less than $0.1$.\relax }{figure.caption.8}{}}
\citation{kleinberg_hubs_1999}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Social Network Features}{13}{subsection.3.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The adjacency matrix representing the social connections of the center. This graph is very well connected with just one component. Nonetheless, there are groups who never exchanged a single email.\relax }}{14}{figure.caption.9}}
\newlabel{fig:adj_matrix}{{3.3}{14}{The adjacency matrix representing the social connections of the center. This graph is very well connected with just one component. Nonetheless, there are groups who never exchanged a single email.\relax }{figure.caption.9}{}}
\citation{page_pagerank_1999}
\citation{lind_cycles_2005}
\citation{saramaki_generalizations_2007}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Histogram of hubs from social graph by job title. Note that directors on average have the highest hub score and graduate students have the lowest. In fact, three out of eight directors can be identified by filtering samples on full graph hubs values $>0.6$. \relax }}{15}{figure.caption.10}}
\newlabel{fig:social_ex_hist}{{3.4}{15}{Histogram of hubs from social graph by job title. Note that directors on average have the highest hub score and graduate students have the lowest. In fact, three out of eight directors can be identified by filtering samples on full graph hubs values $>0.6$. \relax }{figure.caption.10}{}}
\citation{borgatti2011analyzing}
\citation{brandes2005centrality}
\citation{estrada2008communicability}
\citation{newman2001scientific}
\citation{Breiman2001}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Algorithm Design}{17}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Algorithm}{{4}{17}{Algorithm Design}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Tools}{17}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Model Selection}{17}{section.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example random tree of depth 3 to demonstrate how a few rules can be used to find significant class divisions.\relax }}{18}{figure.caption.11}}
\newlabel{fig:ex_tree}{{4.1}{18}{Example random tree of depth 3 to demonstrate how a few rules can be used to find significant class divisions.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Feature Selection}{19}{section.4.3}}
\newlabel{eq:info_gained}{{4.3}{19}{Feature Selection}{equation.4.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Top 20 features ranked by the information gain.\relax }}{21}{table.caption.12}}
\newlabel{tab:ranked_feats}{{4.1}{21}{Top 20 features ranked by the information gain.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Performance Analysis}{22}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Performance}{{5}{22}{Performance Analysis}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Classification Results}{22}{section.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces The Random Forest algorithm was extremely accurate even for very uneven class sizes. Note that all members of 4 classes were labelled perfectly. There were only 2 errors out of 69 employees, both of which for employees who did not provide emails for the study.\relax }}{23}{figure.caption.13}}
\newlabel{fig:result_hist}{{5.1}{23}{The Random Forest algorithm was extremely accurate even for very uneven class sizes. Note that all members of 4 classes were labelled perfectly. There were only 2 errors out of 69 employees, both of which for employees who did not provide emails for the study.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Prediction accuracy compared to number of features used for analysis. Note that the accuracy is still very high, 95.6\%, when only twenty features are used. The outcome of using only the top twenty features produces three classification errors, only one more than using the full set of 98 features. \relax }}{24}{figure.caption.14}}
\newlabel{fig:feat_analysis}{{5.2}{24}{Prediction accuracy compared to number of features used for analysis. Note that the accuracy is still very high, 95.6\%, when only twenty features are used. The outcome of using only the top twenty features produces three classification errors, only one more than using the full set of 98 features. \relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Hierarchy Analysis}{25}{section.5.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Future Work}{26}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{FutureWork}{{6}{26}{Future Work}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Generalization}{26}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Deep Learning}{26}{section.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusions}{27}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Conclusions}{{7}{27}{Conclusions}{chapter.7}{}}
\bibstyle{ieeetr}
\bibdata{bib}
\bibcite{radicati_emails_2015}{1}
\bibcite{fisher_revisiting_2006}{2}
\bibcite{klimt_introducing_2004}{3}
\bibcite{gaber_e-mail_2016}{4}
\bibcite{shams_classifying_2013}{5}
\bibcite{he_novel_2014}{6}
\bibcite{keila_structure_2005}{7}
\bibcite{sofershtein_predicting_2015}{8}
\bibcite{hu_towards_2012}{9}
\bibcite{nordbo_data_2014}{10}
\bibcite{waterman_big_2014}{11}
\bibcite{namata_inferring_2006}{12}
\bibcite{shetty_enron_2004}{13}
\bibcite{tang_email_2013}{14}
\bibcite{yelupula_social_2008}{15}
\bibcite{martin_analyzing_2005}{16}
\bibcite{wasserman_social_1994}{17}
\bibcite{freeman_set_1977}{18}
\bibcite{tyler_email_2003}{19}
\bibcite{wilson_discovery_2009}{20}
\bibcite{rowe_automated_2007}{21}
\bibcite{shetty_status_2004}{22}
\bibcite{kleinberg_hubs_1999}{23}
\bibcite{page_pagerank_1999}{24}
\bibcite{lind_cycles_2005}{25}
\bibcite{saramaki_generalizations_2007}{26}
\bibcite{borgatti2011analyzing}{27}
\bibcite{brandes2005centrality}{28}
\bibcite{estrada2008communicability}{29}
\bibcite{newman2001scientific}{30}
\bibcite{Breiman2001}{31}
